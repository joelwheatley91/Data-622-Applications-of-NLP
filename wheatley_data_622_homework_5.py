# -*- coding: utf-8 -*-
"""Wheatley DATA 622 Homework 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fKV7P20mZEuOFYI-ziZrQYlt5cPtl5NM

Homework 4

Data 622

Joel Wheatley

9/28/25

DATA 622 | HOMEWORK 5
Questions

Use the following article https://www.usatoday.com/story/news/politics/2025/06/13/pete-
hegseth-pentagon-invade-greenland-plan/84188458007/.

1. Extract and print an extractive summary using the TextRank method.
2. Extract and print an extractive summary using a frequency-based sentence scoring
method.
3. Generate and print an abstractive summary using a pretrained transformer model
(e.g., BART or T5).
4. Print a Lead-3 summary (the first three sentences of the article).
5. Print a manual compression summary, limiting the result to about 20% of the
original sentences.
6. Use an LLM to summary the text.
"""

#!pip install cloudscraper
#!pip install pytextrank

from bs4 import BeautifulSoup
import requests
import cloudscraper
import spacy
import pytextrank

url = "https://www.usatoday.com/story/news/politics/2025/06/13/pete-hegseth-pentagon-invade-greenland-plan/84188458007/"
url = "https://www.thebanner.com/community/local-news/penn-north-overdoses-baltimore-YI2IL2M7YBGJNHD4QDAC4UN2SE/"
url = "https://www.thebanner.com/community/transportation/maryland-ev-charging-stations-XFHKA5CNB5A73CW2ZY2XLFUYBU/"

r = requests.get(url, auth=('user', 'pass'))

#Set up the scraper (not using request because of access restriction)
#scraper = cloudscraper.create_scraper()
#r = scraper.get(url)

#r.status_code

soup = BeautifulSoup(r.text, 'html.parser')

text = " ".join([p.get_text() for p in soup.find_all("p")][:-11])
#print(text)

for p in soup.find_all("p"):
  print(p.get_text())
  print("New Paragraph\n")

"""Extract and print an extractive summary using the TextRank method."""

from nltk.tokenize import word_tokenize, sent_tokenize
import nltk, math, re
import numpy as np
#nltk.download('stopwords')
from nltk.corpus import stopwords
#nltk.download('punkt')
#nltk.download('punkt_tab')

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("textrank")
#text = soup.find("article").get_text()
t = text.lower()
t = t.replace(',','')
t = t.replace('’','')
doc = nlp(t)
#for p in doc._.phrases[:10]:
#    print("{:.4f} {:5d}  {}".format(p.rank, p.count, p.text))

sentences = sent_tokenize(text)
for sent in sentences:
  print(sent)
t = text.lower()
t = t.replace(',','')
t = t.replace('’','')
filtered_sentences = sent_tokenize(t)
for i in range(len(filtered_sentences)):
  filtered_sentences[i] = filtered_sentences[i].replace('.','')

t = t.replace('.','')
filtered_words = word_tokenize(t)
word_freq = {}
for word in filtered_words:
  if word not in stopwords.words('english'):
    if word not in word_freq:
      word_freq[word] = 1
    else:
      word_freq[word] += 1

max_freq = max(word_freq.values())

for word in word_freq:
  word_freq[word] /= max_freq

#print(word_freq)

"""
for phrase in doc._.phrases:
    print(phrase.text)
    print(phrase.rank, phrase.count)
    print(phrase.chunks)
"""

#for sent in doc._.textrank.summary(limit_phrases=10, limit_sentences=3):
#  print(sent)

sent_scores = []
for sent in filtered_sentences:
  sent_scores.append(0)
  for word in word_tokenize(sent):
    if word not in stopwords.words('english'):
      sent_scores[-1] += word_freq[word]

sent_scores = np.asarray(sent_scores)

summary_ratio = 0.2
summary_len = int(summary_ratio*len(sentences))
#summary_len = 3
summary = ''
summary_idx = np.argsort(sent_scores)[-summary_len:]
summary_idx = summary_idx[::-1]
print(len(sentences))
print("")
print("Using Textrank")
for i in summary_idx:
  print(sentences[i])

"""Extract and print an extractive summary using a frequency-based sentence scoring method."""

#Use TF-IDF to calculate this
from sklearn.feature_extraction.text import TfidfVectorizer

vec = TfidfVectorizer(stop_words='english')
score_matrix = vec.fit_transform(sentences)

#Create a score matrix
#.A1 looks only at the first column, instead of the sparse matrix
sent_score = score_matrix.sum(axis=1).A1
#print(sent_score)
#Create an index of the sentences by TF-IDF rank
sent_index = np.argsort(np.array(sent_score))

sorted_sentences = [sentences[i] for i in sent_index]
print("")
print("TF-IDF Ranking")
for sent in sorted_sentences[-summary_len:]:
  print(sent)

"""Generate and print an abstractive summary using a pretrained transformer model (e.g., BART or T5).

Print a Lead-3 summary (the first three sentences of the article).
"""

#Split the sentences, and pull the first 3
print("")
print("3-Lead")
for sent in sentences[:3]:
  print(sent)

"""Print a manual compression summary, limiting the result to about 20% of the original sentences.

Use an LLM to summary the text.

Below is a summary of the article by chatGPT

The Pentagon says it has contingency plans for Greenland but denies preparing an invasion, despite President Trump’s interest in acquiring the island for strategic and economic reasons. U.S. officials argue that control of Greenland is key to countering Chinese and Russian influence and accessing valuable minerals. Vice President JD Vance has criticized Denmark’s ability to defend the territory, while Danish leaders insist Greenland’s sovereignty rests with its people. In a shift that has unsettled European allies, the Pentagon is expected to transfer oversight of Greenland from European to Northern Command.
"""