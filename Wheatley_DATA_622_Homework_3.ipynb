{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Applications of Natural Language Processing\n",
        "\n",
        "Homework Three\n",
        "\n",
        "Joel Wheatley\n",
        "\n",
        "9/21/2025"
      ],
      "metadata": {
        "id": "3zMNGN5JZFe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the File\n",
        "\n",
        "Read the content of https://www.washingtonpost.com/world/2025/06/13/air-\n",
        "india-plane-crash-survivor-vishwash-kumar-ramesh/ into a Python variable.\n",
        "\n",
        "Load the first 700 characters.\n",
        "\n",
        "2. Split the Text into Sentences\n",
        "Split the text into sentences using NLTK’s sentence tokenizer.\n",
        "\n",
        "3. Load a Pre-trained Embedding Model\n",
        "Load a pre-trained sentence embedding model (such as all-MiniLM-L6-v2 from\n",
        "the sentence-transformers library or anyone of your choice).\n",
        "Secondly, use TF/IDF to vectorize the first ten sentences.\n",
        "\n",
        "4. Embed Each Sentence\n",
        "Generate an embedding for each sentence.\n",
        "Print the shape of the embedding for the first sentence.\n",
        "\n",
        "5. Compute Similarity Between Sentences\n",
        "Compute the cosine similarity between the embeddings of the first and second\n",
        "sentences.\n",
        "Print the similarity score."
      ],
      "metadata": {
        "id": "rnfQKu0WZA7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Read the File\n",
        "Read the content of https://www.washingtonpost.com/world/2025/06/13/air- india-plane-crash-survivor-vishwash-kumar-ramesh/ into a Python variable.\n",
        "\n",
        "Load the first 700 characters."
      ],
      "metadata": {
        "id": "r1MkFO17ZP7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I was unable to access the article in python due to the pay wall.\n",
        "#Uncommenting the below code should run the program as intended, if able to get through the paywall.\n",
        "'''\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'https://www.washingtonpost.com/world/2025/06/13/air-india-plane-crash-survivor-vishwash-kumar-ramesh/'\n",
        "#url = 'https://www.cnn.com/2025/06/12/india/air-india-crash-survivor-vishwash-kumar-ramesh-intl-latam'\n",
        "r = requests.get(url)\n",
        "\n",
        "print(r.status_code)\n",
        "\n",
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "text = soup.get_text()\n",
        "print(text[:700])\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "f2DQB8LFZQQW",
        "outputId": "5ea0ed98-6039-4c5c-99b6-510a3e0958a7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom bs4 import BeautifulSoup\\nimport requests\\n\\nurl = 'https://www.washingtonpost.com/world/2025/06/13/air-india-plane-crash-survivor-vishwash-kumar-ramesh/'\\n#url = 'https://www.cnn.com/2025/06/12/india/air-india-crash-survivor-vishwash-kumar-ramesh-intl-latam'\\nr = requests.get(url)\\n\\nprint(r.status_code)\\n\\nsoup = BeautifulSoup(r.text, 'html.parser')\\ntext = soup.get_text()\\nprint(text[:700])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import the standard text cleaning libraries, in case we want to reun a regex on the text later.\n",
        "#Similarly, download punctuation and stopword cleaning repositories in case we want to use them later.\n",
        "#Note: We won't for this particulary exercise, but I want to get in the habit of including them for future exercises.\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtNAWDL0fo0b",
        "outputId": "2e6033a1-6a6d-4bc7-eb11-82a667b7d6f0"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Comment out below if requests is working\n",
        "\n",
        "\n",
        "#I was having issues accessing the article in Python due to the Paywall\n",
        "#Instead, to get the text, I copy and pasted the article into a text file called HW 3.txt, which I opened and assigned to \"text\"\n",
        "from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "with open('HW 3.txt', 'r') as f:\n",
        "    text = f.read()#.replace('\\n',' ').replace('  ',' ')\n",
        "print(text[:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVEWbGxxeKUu",
        "outputId": "b1d34f6f-9e36-47a0-b6b8-651869eb6cc7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ‘miracle’ of the sole passenger who survived the Air India flight\n",
            "Viswashkumar Ramesh, on Air India Flight 171 in seat 11A, was the only survivor after the plane crashed in Ahmedabad. An expert called his escape “extraordinary.”\n",
            "\n",
            "Updated June 13, 2025\n",
            "4 min\n",
            "\n",
            "Summary\n",
            "\n",
            "\n",
            "\n",
            "395\n",
            "\n",
            "An investigation team inspects the wreckage of Air India Flight 171 on Friday, a day after it crashed in a residential area near the airport in Ahmedabad, India. (Sam Panthaky/AFP/Getty Images)\n",
            "By Vivian Ho and Supriya Kumar\n",
            "Only one person on board Air India Flight 171 survived — British national Viswashkumar Ramesh, who could be seen limping past a crowd of shocked rescuers toward an ambulance shortly after the cras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Split the Text into Sentences Split the text into sentences using NLTK’s sentence tokenizer."
      ],
      "metadata": {
        "id": "IaCSwupIfWOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the nltk sentence tokenizer\n",
        "#An error was being thrown saying 'punkt_tab' was missing, so I downloaded that on top of 'punkt' and that resolved that error.\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#Tokenize the text into sentences\n",
        "sentences = sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6P7PO2pfVyE",
        "outputId": "e2180e35-43ab-426f-d2ff-f7fb3fdff8c3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(sentences[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yof6-AKhMJl",
        "outputId": "295be607-6483-4257-a550-078ab03fe584"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ‘miracle’ of the sole passenger who survived the Air India flight\n",
            "Viswashkumar Ramesh, on Air India Flight 171 in seat 11A, was the only survivor after the plane crashed in Ahmedabad.\n",
            "An expert called his escape “extraordinary.”\n",
            "\n",
            "Updated June 13, 2025\n",
            "4 min\n",
            "\n",
            "Summary\n",
            "\n",
            "\n",
            "\n",
            "395\n",
            "\n",
            "An investigation team inspects the wreckage of Air India Flight 171 on Friday, a day after it crashed in a residential area near the airport in Ahmedabad, India.\n",
            "(Sam Panthaky/AFP/Getty Images)\n",
            "By Vivian Ho and Supriya Kumar\n",
            "Only one person on board Air India Flight 171 survived — British national Viswashkumar Ramesh, who could be seen limping past a crowd of shocked rescuers toward an ambulance shortly after the crash killed the other 241 passengers and crew members, as well as dozens of people on the ground in Ahmedabad.\n",
            "Ramesh, 40, has been described as the “miracle in seat 11A” in British media, and several top Indian officials — including Prime Minister Narendra Modi — have visited him in the hospital.\n",
            "“I don’t know how I survived,” Ramesh said in an interview from his hospital bed with broadcaster Doordarshan on Friday, with one arm heavily bandaged and a bloodied cut under his eye.\n",
            "“I was on the side of the plane that crashed on the ground floor of the hostel,” he said, adding that both the emergency door and his seat were broken in the crash.\n",
            "“When the door broke, I saw that there was some space for me to get out.\n",
            "That’s how I escaped.”\n",
            "\n",
            "“The other side of the plane was stuck in the hostel wall.\n",
            "No one would have been able to escape from that side,” he added.\n",
            "The plane crashed into a college hostel shortly after takeoff.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a Pre-trained Embedding Model Load a pre-trained sentence embedding model (such as all-MiniLM-L6-v2 from the sentence-transformers library or anyone of your choice). Secondly, use TF/IDF to vectorize the first ten sentences.\n"
      ],
      "metadata": {
        "id": "aQEYWZ9fi2Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install sentence-transformers\n",
        "#Import transformer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "transform_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#Initilize vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "#Fit vectorizer to first 10 sentences.\n",
        "vectorizer.fit_transform(sentences[:10])\n",
        "#Sanity Check\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Q9TyTJlFsR",
        "outputId": "132213b7-8dd9-4ab0-b129-02c86f264bd3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['11a' '13' '171' '2025' '241' '395' '40' 'able' 'added' 'adding' 'afp'\n",
            " 'after' 'ahmedabad' 'air' 'airport' 'ambulance' 'an' 'and' 'area' 'arm'\n",
            " 'as' 'bandaged' 'be' 'bed' 'been' 'bloodied' 'board' 'both' 'british'\n",
            " 'broadcaster' 'broke' 'broken' 'by' 'called' 'college' 'could' 'crash'\n",
            " 'crashed' 'crew' 'crowd' 'cut' 'day' 'described' 'don' 'door'\n",
            " 'doordarshan' 'dozens' 'emergency' 'escape' 'escaped' 'expert'\n",
            " 'extraordinary' 'eye' 'flight' 'floor' 'for' 'friday' 'from' 'get'\n",
            " 'getty' 'ground' 'has' 'have' 'he' 'heavily' 'him' 'his' 'ho' 'hospital'\n",
            " 'hostel' 'how' 'images' 'in' 'including' 'india' 'indian' 'inspects'\n",
            " 'interview' 'into' 'investigation' 'it' 'june' 'killed' 'know' 'kumar'\n",
            " 'limping' 'me' 'media' 'members' 'min' 'minister' 'miracle' 'modi'\n",
            " 'narendra' 'national' 'near' 'no' 'of' 'officials' 'on' 'one' 'only'\n",
            " 'other' 'out' 'panthaky' 'passenger' 'passengers' 'past' 'people'\n",
            " 'person' 'plane' 'prime' 'ramesh' 'rescuers' 'residential' 'said' 'sam'\n",
            " 'saw' 'seat' 'seen' 'several' 'shocked' 'shortly' 'side' 'sole' 'some'\n",
            " 'space' 'stuck' 'summary' 'supriya' 'survived' 'survivor' 'takeoff'\n",
            " 'team' 'that' 'the' 'there' 'to' 'top' 'toward' 'under' 'updated'\n",
            " 'visited' 'viswashkumar' 'vivian' 'wall' 'was' 'well' 'were' 'when' 'who'\n",
            " 'with' 'would' 'wreckage']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Embed Each Sentence\n",
        "\n",
        "Generate an embedding for each sentence.\n",
        "\n",
        "Print the shape of the embedding for the first sentence."
      ],
      "metadata": {
        "id": "sPyffesvvqeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the embedding, then access and print hte first sentence in it.\n",
        "embeddings = transform_model.encode(sentences)\n",
        "print(embeddings[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCYzeYIKxIWP",
        "outputId": "2d090d09-5498-493f-b4a8-6c6abdfa19d0"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(384,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compute Similarity Between Sentences\n",
        "\n",
        "Compute the cosine similarity between the embeddings of the first and second sentences. Print the similarity score."
      ],
      "metadata": {
        "id": "c-LoMi3pxrOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Had to look up the specific funciton to do this\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#This will give us a matrix, the similarity score between the 1st and 2nd sentences will be the 1st row, second column [0,1]\n",
        "similarity_score = cosine_similarity([embeddings[0], embeddings[1]])\n",
        "print(similarity_score)\n",
        "print(similarity_score[0,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKqb-96Yv0Bv",
        "outputId": "8663a71c-8b3b-4adb-ef01-0a49d36c533c"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.9999999 0.5869269]\n",
            " [0.5869269 1.       ]]\n",
            "0.5869269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5F17OiMcKIrQ"
      }
    }
  ]
}