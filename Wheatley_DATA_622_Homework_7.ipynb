{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f030be-8c8f-4906-a780-fd6ef9fe4ad7",
   "metadata": {},
   "source": [
    "Extract the article of Sam Altman’s interview https://venturebeat.com/ai/sam-altman-at-\r\n",
    "ted-2025-inside-the-most-uncomfortable-and-important-ai-interview-of-the-year/.\r\n",
    "\r\n",
    "Instructions:\r\n",
    "1. Use an LLM to summarize the article.\r\n",
    "2. Identify the key topics, and the sentiment. Is the sentiment measured by the LLM\r\n",
    "different from one using a classification model such as Naïve Bayes or Support\r\n",
    "Vector Machine?\r\n",
    "3. What is the general emotion of the article?\r\n",
    "4. What is the main theme of the article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d2fbb5-1e31-4628-aab7-4a6e0a89b9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Due to troubles pulling the info from the website, I copied and pasted the article into a text file on my machine\n",
    "file_name = \"HW_7_text_file.txt\"\n",
    "#url = https://venturebeat.com/ai/sam-altman-at-ted-2025-inside-the-most-uncomfortable-and-important-ai-interview-of-the-year\n",
    "\n",
    "with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f763415c-5547-4e78-9c3a-fc08bc3423c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Altman at TED 2025: Inside the most uncomfortable — and important — AI interview of the year\n",
      "Michael Nuñez\n",
      "April 15, 2025\n",
      "Host Chris Anderson and Sam Altman speak at SESSION 11 at TED 2025: Humanity Reimagined. April 7-11, 2025, Vancouver, BC. Photo: Jason Redmond / TED\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OpenAI CEO Sam Altman revealed that his company has grown to 800 million weekly active users and is experiencing \"unbelievable\" growth rates, during a sometimes tense interview at the TED 2025 conference in Vancouver last week.\n",
      "\n",
      "\"I have never seen growth in any company, one that I've been involved with or not, like this,\" Altman told TED head Chris Anderson during their on-stage conversation. \"The growth of ChatGPT — it is really fun. I feel deeply honored. But it is crazy to live through, and our teams are exhausted and stressed.\"\n",
      "\n",
      "The interview, which closed out the final day of TED 2025: Humanity Reimagined, showcased not just OpenAI's skyrocketing success but also the increasing scrutiny the company faces as its technology transforms society at a pace that alarms even some of its supporters.\n",
      "\n",
      "\n",
      "'Our GPUs are melting': OpenAI struggles to scale amid unprecedented demand\n",
      "Altman painted a picture of a company struggling to keep up with its own success, noting that OpenAI's GPUs are \"melting\" due to the popularity of its new image generation features. \"All day long, I call people and beg them to give us their GPUs. We are so incredibly constrained,\" he said.\n",
      "\n",
      "\n",
      "This exponential growth comes as OpenAI is reportedly considering launching its own social network to compete with Elon Musk's X, according to CNBC. Altman neither confirmed nor denied these reports during the TED interview.\n",
      "\n",
      "The company recently closed a $40 billion funding round, valuing it at $300 billion — the largest private tech funding in history — and this influx of capital will likely help address some of these infrastructure challenges.\n",
      "\n",
      "From non-profit to $300 billion giant: Altman responds to 'Ring of Power' accusations\n",
      "Throughout the 47-minute conversation, Anderson repeatedly pressed Altman on OpenAI's transformation from a non-profit research lab to a for-profit company with a $300 billion valuation. Anderson voiced concerns shared by critics, including Elon Musk, who has suggested Altman has been \"corrupted by the Ring of Power,\" referencing \"The Lord of the Rings.\"\n",
      "\n",
      "Altman defended OpenAI's path: \"Our goal is to make AGI and distribute it, make it safe for the broad benefit of humanity. I think by all accounts, we have done a lot in that direction. Clearly, our tactics have shifted over time... We didn't think we would have to build a company around this. We learned a lot about how it goes and the realities of what these systems were going to take from capital.\"\n",
      "\n",
      "When asked how he personally handles the enormous power he now wields, Altman responded: \"Shockingly, the same as before. I think you can get used to anything step by step... You're the same person. I'm sure I'm not in all sorts of ways, but I don't feel any different.\"\n",
      "\n",
      "'Divvying up revenue': OpenAI plans to pay artists whose styles are used by AI\n",
      "One of the most concrete policy announcements from the interview was Altman's acknowledgment that OpenAI is working on a system to compensate artists whose styles are emulated by AI.\n",
      "\n",
      "\"I think there are incredible new business models that we and others are excited to explore,\" Altman said when pressed about apparent IP theft in AI-generated images. \"If you say, 'I want to generate art in the style of these seven people, all of whom have consented to that,' how do you divvy up how much money goes to each one?\"\n",
      "\n",
      "Currently, OpenAI's image generator refuses requests to mimic the style of living artists without consent, but will generate art in the style of movements, genres, or studios. Altman suggested a revenue-sharing model could be forthcoming, though details remain scarce.\n",
      "\n",
      "Autonomous AI agents: The 'most consequential safety challenge' OpenAI has faced\n",
      "The conversation grew particularly tense when discussing \"agentic AI\" — autonomous systems that can take actions on the internet on a user's behalf. OpenAI's new \"Operator\" tool allows AI to perform tasks like booking restaurants, raising concerns about safety and accountability.\n",
      "\n",
      "Anderson challenged Altman: \"A single person could let that agent out there, and the agent could decide, 'Well, in order to execute on that function, I got to copy myself everywhere.' Are there red lines that you have clearly drawn internally, where you know what the danger moments are?\"\n",
      "\n",
      "Altman referenced OpenAI's \"preparedness framework\" but provided few specifics about how the company would prevent misuse of autonomous agents.\n",
      "\n",
      "\"AI that you give access to your systems, your information, the ability to click around on your computer... when they make a mistake, it's much higher stakes,\" Altman acknowledged. \"You will not use our agents if you do not trust that they're not going to empty your bank account or delete your data.\"\n",
      "\n",
      "'14 definitions from 10 researchers': Inside OpenAI's struggle to define AGI\n",
      "In a revealing moment, Altman admitted that even within OpenAI, there's no consensus on what constitutes artificial general intelligence (AGI) — the company's stated goal.\n",
      "\n",
      "\"It's like the joke, if you've got 10 OpenAI researchers in a room and asked to define AGI, you'd get 14 definitions,\" Altman said.\n",
      "\n",
      "He suggested that rather than focusing on a specific moment when AGI arrives, we should recognize that \"the models are just going to get smarter and more capable and smarter and more capable on this long exponential... We're going to have to contend and get wonderful benefits from this incredible system.\"\n",
      "\n",
      "Loosening the guardrails: OpenAI's new approach to content moderation\n",
      "Altman also disclosed a significant policy change regarding content moderation, revealing that OpenAI has loosened restrictions on its image generation models.\n",
      "\n",
      "\"We've given the users much more freedom on what we would traditionally think about as speech harms,\" he explained. \"I think part of model alignment is following what the user of a model wants it to do within the very broad bounds of what society decides.\"\n",
      "\n",
      "This shift could signal a broader move toward giving users more control over AI outputs, potentially aligning with Altman's expressed preference for letting the hundreds of millions of users — rather than \"small elite summits\" — determine appropriate guardrails.\n",
      "\n",
      "\"One of the cool new things about AI is our AI can talk to everybody on Earth, and we can learn the collective value preference of what everybody wants, rather than have a bunch of people who are blessed by society to sit in a room and make these decisions,\" Altman said.\n",
      "\n",
      "'My kid will never be smarter than AI': Altman's vision of an AI-powered future\n",
      "The interview concluded with Altman reflecting on the world his newborn son will inherit — one where AI will exceed human intelligence.\n",
      "\n",
      "\"My kid will never be smarter than AI. They will never grow up in a world where products and services are not incredibly smart, incredibly capable,\" he said. \"It'll be a world of incredible material abundance... where the rate of change is incredibly fast and amazing new things are happening.\"\n",
      "\n",
      "Anderson closed with a sobering observation: \"Over the next few years, you're going to have some of the biggest opportunities, the biggest moral challenges, the biggest decisions to make of perhaps any human in history.\"\n",
      "\n",
      "The billion-user balancing act: How OpenAI navigates power, profit, and purpose\n",
      "Altman's TED appearance comes at a critical juncture for OpenAI and the broader AI industry. The company faces mounting legal challenges, including copyright lawsuits from authors and publishers, while simultaneously pushing the boundaries of what AI can do.\n",
      "\n",
      "Recent advancements like ChatGPT's viral image generation feature and video generation tool Sora have demonstrated capabilities that seemed impossible just months ago. At the same time, these tools have sparked debates about copyright, authenticity, and the future of creative work.\n",
      "\n",
      "Altman's willingness to engage with difficult questions about safety, ethics, and the societal impact of AI shows an awareness of the stakes involved. However, critics may note that concrete answers on specific safeguards and policies remained elusive throughout the conversation.\n",
      "\n",
      "The interview also revealed the competing tensions at the heart of OpenAI's mission: moving fast to advance AI technology while ensuring safety; balancing profit motives with societal benefit; respecting creative rights while democratizing creative tools; and navigating between elite expertise and public preference.\n",
      "\n",
      "As Anderson noted in his final comment, the decisions Altman and his peers make in the coming years may have unprecedented impacts on humanity's future. Whether OpenAI can live up to its stated mission of ensuring \"all of humanity benefits from artificial general intelligence\" remains to be seen.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df44ba3-c956-4b16-a1aa-67ac2fb3faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get open ai key from .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import openai\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c49fbf-a70f-45b3-a50c-f861bbddafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article discusses OpenAI CEO Sam Altman's interview at the TED 2025 conference in Vancouver, where he revealed the company's massive growth to 800 million weekly active users and a $300 billion valuation. Altman highlighted the challenges of scaling to meet the unprecedented demand, including the strain on their GPUs and plans for a potential social network launch. The conversation also addressed criticisms about OpenAI's evolution from a non-profit to a for-profit entity and the need for compensating artists whose styles are used by AI.\n",
      "\n",
      "Altman discussed the safety challenges posed by autonomous AI agents and OpenAI's struggle to define artificial general intelligence (AGI). The company's approach to content moderation has shifted to give users more control over AI outputs. Altman envisioned a future where AI surpasses human intelligence, acknowledging the moral dilemmas and decisions that come with it. The interview underscored the complex balancing act OpenAI faces between power, profit, and purpose, with implications for the future of AI technology and its societal impact.\n"
     ]
    }
   ],
   "source": [
    "#1. Use an LLM to summarize the article.\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=[\n",
    "                                              {\"role\":\"system\",\"content\":\"You are an article analyst\"},\n",
    "                                              {\"role\": \"user\", \"content\": f\"Summerize the following article: {text}\"}\n",
    "                                          ])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57facd70-5c6d-4c93-97a1-05a5dee693b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key topics:\n",
      "1. OpenAI's rapid growth and challenges in scaling its operations\n",
      "2. OpenAI's transition from a non-profit to a for-profit company and the ethical implications\n",
      "3. Plans for a social network launch and revenue sharing with artists\n",
      "4. Concerns and challenges related to autonomous AI agents\n",
      "5. The definition and implications of artificial general intelligence (AGI)\n",
      "6. Changes in OpenAI's content moderation policies and focus on user preferences\n",
      "7. Altman's vision of an AI-powered future and the potential impact on society\n",
      "8. The critical juncture and legal challenges faced by OpenAI in the AI industry\n",
      "\n",
      "Sentiment:\n",
      "Overall, the sentiment in the text is mixed. There are mentions of remarkable growth and achievements by OpenAI, but there is also a sense of challenges, scrutiny, and tensions surrounding the company's operations and decisions. The text touches on ethical concerns, potential misuse of AI technology, uncertainty around defining and managing AGI, and the evolving policies of OpenAI on content moderation. Critics may argue that specific safeguards and policies are lacking, highlighting the need for more concrete answers from OpenAI. The interview reflects the complex balance between pursuing technological advancements, ensuring safety and ethics, and addressing societal implications, indicating a cautious and critical stance towards OpenAI's role in shaping the future of AI.\n"
     ]
    }
   ],
   "source": [
    "#2. Identify the key topics, and the sentiment. Is the sentiment measured by the LLM\n",
    "#different from one using a classification model such as Naïve Bayes or Support\n",
    "#Vector Machine?\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=[\n",
    "                                              {\"role\":\"system\",\"content\":\"You are an article analyst\"},\n",
    "                                              {\"role\": \"user\", \"content\": f\"Identify the key topics, and the sentiment of the following text: {text}\"}\n",
    "                                          ])\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc633d14-f4f6-4026-86e3-7d5713d3f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\joelw\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from textblob) (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\joelw\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043da9d3-818f-4b2b-b8c5-62657c1b3fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 13, in <module>\n",
      "    from nltk.chunk.util import ChunkScore\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py\", line 12, in <module>\n",
      "    from nltk.tag.mapping import map_tag\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py\", line 72, in <module>\n",
      "    from nltk.tag.sequential import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py\", line 26, in <module>\n",
      "    from nltk.classify import NaiveBayesClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py\", line 97, in <module>\n",
      "    from nltk.classify.scikitlearn import SklearnClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py\", line 38, in <module>\n",
      "    from sklearn.feature_extraction import DictVectorizer\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 38, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py\", line 46, in __getattr__\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 13, in <module>\n",
      "    from nltk.chunk.util import ChunkScore\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py\", line 12, in <module>\n",
      "    from nltk.tag.mapping import map_tag\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py\", line 72, in <module>\n",
      "    from nltk.tag.sequential import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py\", line 26, in <module>\n",
      "    from nltk.classify import NaiveBayesClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py\", line 97, in <module>\n",
      "    from nltk.classify.scikitlearn import SklearnClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py\", line 38, in <module>\n",
      "    from sklearn.feature_extraction import DictVectorizer\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py\", line 46, in __getattr__\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 13, in <module>\n",
      "    from nltk.chunk.util import ChunkScore\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py\", line 12, in <module>\n",
      "    from nltk.tag.mapping import map_tag\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py\", line 72, in <module>\n",
      "    from nltk.tag.sequential import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py\", line 26, in <module>\n",
      "    from nltk.classify import NaiveBayesClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py\", line 97, in <module>\n",
      "    from nltk.classify.scikitlearn import SklearnClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py\", line 38, in <module>\n",
      "    from sklearn.feature_extraction import DictVectorizer\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 52, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n",
      "AttributeError: _ARRAY_API not found\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 13, in <module>\n",
      "    from nltk.chunk.util import ChunkScore\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py\", line 12, in <module>\n",
      "    from nltk.tag.mapping import map_tag\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py\", line 72, in <module>\n",
      "    from nltk.tag.sequential import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py\", line 26, in <module>\n",
      "    from nltk.classify import NaiveBayesClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py\", line 97, in <module>\n",
      "    from nltk.classify.scikitlearn import SklearnClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py\", line 38, in <module>\n",
      "    from sklearn.feature_extraction import DictVectorizer\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 61, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 66, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 61, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py\", line 46, in __getattr__\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 13, in <module>\n",
      "    from nltk.chunk.util import ChunkScore\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py\", line 12, in <module>\n",
      "    from nltk.tag.mapping import map_tag\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py\", line 72, in <module>\n",
      "    from nltk.tag.sequential import (\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py\", line 26, in <module>\n",
      "    from nltk.classify import NaiveBayesClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py\", line 97, in <module>\n",
      "    from nltk.classify.scikitlearn import SklearnClassifier\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py\", line 38, in <module>\n",
      "    from sklearn.feature_extraction import DictVectorizer\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 421, in <module>\n",
      "    import pyarrow\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py\", line 46, in __getattr__\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\__init__.py\", line 1, in <module>\n",
      "    from .blob import Blobber, Sentence, TextBlob, Word, WordList\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\textblob\\blob.py\", line 27, in <module>\n",
      "    import nltk\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py\", line 147, in <module>\n",
      "    from nltk.chunk import *\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py\", line 155, in <module>\n",
      "    from nltk.chunk.api import ChunkParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py\", line 15, in <module>\n",
      "    from nltk.parse import ParserI\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\parse\\__init__.py\", line 100, in <module>\n",
      "    from nltk.parse.transitionparser import TransitionParser\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\nltk\\parse\\transitionparser.py\", line 18, in <module>\n",
      "    from sklearn import svm\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 20, in <module>\n",
      "    from .utils._missing import is_scalar_nan\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\fixes.py\", line 421, in <module>\n",
      "    import pyarrow\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\joelw\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py\", line 46, in __getattr__\n",
      "    raise ImportError(msg)\n",
      "ImportError: \n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\joelw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd7fa46e-0797-4be4-a359-b90802926256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Sentiment: Sentiment(polarity=0.12286232482207061, subjectivity=0.512573907965857)\n",
      "0.13333333333333333\n",
      "0.0\n",
      "-0.17916666666666664\n",
      "0.0\n",
      "0.3\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(text)\n",
    "#print(blob.sentences)\n",
    "#polarity of the article\n",
    "print(f\"Article Sentiment: {blob.sentiment}\")\n",
    "\n",
    "#polarity of each sentence\n",
    "for sentence in blob.sentences[:6]:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a40309-bcf3-49b6-9d87-8a70d15af879",
   "metadata": {},
   "source": [
    "The sentiment measured by the LLM is mixed, which is close to the .1228 polarity measured by the textblob, which uses a naive bayes model. The text blob implies it is a more subjective article, while the LLM doesn't account for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c49ecc-e710-4650-97ca-433e17f1fbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The general emotion in the article appears to be a mix of intrigue, tension, and concern. The discussion around OpenAI's rapid growth, challenges in scaling, transformation from non-profit to for-profit, and ethical considerations surrounding AI technology raises questions and uncertainties. There are elements of excitement about the possibilities of AI advancement, but also a clear apprehension about the implications of such progress, especially in areas like content moderation, compensation for artists, and ensuring the safe and responsible use of autonomous AI agents. Overall, the tone of the article suggests a complex and nuanced atmosphere where the immense potential of AI is juxtaposed with the ethical and societal dilemmas it presents.\n"
     ]
    }
   ],
   "source": [
    "#3. What is the general emotion of the article?\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=[\n",
    "                                              {\"role\":\"system\",\"content\":\"You are an article analyst\"},\n",
    "                                              {\"role\": \"user\", \"content\": f\"What is the general emotion: {text}\"}\n",
    "                                          ])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f555193-2a10-4115-82ee-6b4166c4eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main theme of this text appears to be the evolving challenges and ethical considerations surrounding the rapid growth and impact of artificial intelligence technology, particularly as exemplified by the case of OpenAI and its CEO, Sam Altman. The text delves into various aspects, including the company's exponential growth, ethical concerns related to AI, the transformation of OpenAI from a non-profit to a for-profit entity, the need for policies around compensation for artists whose work is emulated by AI, the potential risks of autonomous AI agents, the struggle to define artificial general intelligence (AGI), and the shifting approaches to content moderation in AI models. It highlights the tensions and dilemmas faced by OpenAI in balancing power, profit, and societal impact in the context of AI's rapid advancement and the profound implications it holds for humanity. The text raises important questions about accountability, safety, ethics, and the future of AI technology, portraying a critical examination of the challenges and responsibilities associated with the development and deployment of AI in society.\n"
     ]
    }
   ],
   "source": [
    "#4. What is the main theme of the article?\n",
    "response = client.chat.completions.create(model=\"gpt-3.5-turbo\",\n",
    "                                          messages=[\n",
    "                                              {\"role\":\"system\",\"content\":\"You are an article analyst\"},\n",
    "                                              {\"role\": \"user\", \"content\": f\"What is the main theme of this text: {text}\"}\n",
    "                                          ])\n",
    "print(response.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
